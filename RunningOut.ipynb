{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import itertools\n",
    "import os\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenise_text(text):\n",
    "    pattern = '[\\w]+'\n",
    "    punctuation = re.split(pattern, text)\n",
    "    words = re.findall(pattern, text)\n",
    "    if len(words) == 0:\n",
    "        tokens = punctuation\n",
    "    else:\n",
    "        tokens = list(itertools.chain.from_iterable(zip(punctuation, words)))+[punctuation[-1]]\n",
    "    tokens = [x for x in tokens if x != '']\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_text(text, model, ngram_lengths=list(range(2,6))):\n",
    "    tokens = tokenise_text(text.lower())\n",
    "    for ngram_length in ngram_lengths:\n",
    "        for i in range(len(tokens)-ngram_length+1):\n",
    "            ngram = tokens[i:i+ngram_length]\n",
    "            head, tail = tuple(ngram[:-1]),ngram[-1]\n",
    "            try:\n",
    "                model[head].add(tail)\n",
    "            except KeyError:\n",
    "                model[head] = {tail}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def disable_production(rule, production):\n",
    "    rule.remove(production)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def capitalise(text):\n",
    "    text = re.sub('\\\\bi\\\\b','I',text)\n",
    "    punctuation_pattern = '[\\.\\?\\!]+'\n",
    "    alpha_pattern = '[a-zA-ZþÞȝȜ]'\n",
    "    matches = [0]+[x.end() for x in re.finditer(punctuation_pattern,text)]\n",
    "    if matches == [0]:\n",
    "        return text.capitalize()\n",
    "    matches = [matches[i:i+2] for i in range(len(matches)-1)]\n",
    "    sentences = [text[a:b] for (a,b) in matches]\n",
    "    starts = []\n",
    "    for sentence in sentences:\n",
    "        try:\n",
    "            starts.append(next(re.finditer(alpha_pattern,sentence)).start())\n",
    "        except StopIteration:\n",
    "            starts.append(0)\n",
    "    sentences = [sentence[:start]+sentence[start:].capitalize()\n",
    "                 for (sentence, start) in zip(sentences, starts)]\n",
    "    return ''.join(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def enable_all(model):\n",
    "    for rule in model.values():\n",
    "        disabled_productions = [production for production in rule if not is_enabled(production)]\n",
    "        for dp in disabled_productions:\n",
    "            rule.difference_update(dp)\n",
    "            rule.update([enabled_production(dp) for dp in disabled_productions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def output_text(model, stop_length=50000, ngram_lengths=list(range(2,6))):\n",
    "    tokens = []\n",
    "    length = 0\n",
    "    possibilities = list(itertools.chain.from_iterable(model.values()))\n",
    "    print('output_text(): found {0} possible starting tokens'.format(len(possibilities)))\n",
    "    if(len(possibilities)) == 0:\n",
    "        return '',stop_length+1\n",
    "    tokens.append(random.choice(possibilities))\n",
    "    if(tokens[-1].isalnum()):\n",
    "        length += 1\n",
    "    while length <= stop_length:\n",
    "        most_recent_ngrams = [tuple(tokens[-ngl:]) for ngl in ngram_lengths[::-1]]\n",
    "        found_ngram = False\n",
    "        for ngram in most_recent_ngrams:\n",
    "            try:\n",
    "                rule = model[ngram]\n",
    "            except KeyError:\n",
    "                continue\n",
    "            possibilities = list(rule)\n",
    "            if len(possibilities) == 0:\n",
    "                del model[ngram]\n",
    "                continue\n",
    "            production = random.choice(possibilities)\n",
    "            tokens.append(production)\n",
    "            if random.random() < 0.9:\n",
    "                rule.remove(production)\n",
    "                if len(rule) == 0:\n",
    "                    del model[ngram]\n",
    "            if(tokens[-1].isalnum()):\n",
    "                length += 1\n",
    "            found_ngram = True\n",
    "            break\n",
    "        if not found_ngram:\n",
    "            #print(\"Couldn't find any productions at all, stopping\")\n",
    "            break\n",
    "    punctuation = ['.','!','?',',',';',':']\n",
    "    if tokens[-1][-1] not in punctuation:\n",
    "        tokens.append(random.choice(punctuation))\n",
    "    #enable_all(model)\n",
    "    return capitalise(''.join(tokens)), length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sources\\auld_zimmery.txt...\n"
     ]
    }
   ],
   "source": [
    "model = {}\n",
    "corpora_dir = 'sources'\n",
    "for fn in [path.join(corpora_dir,fn) for fn in os.listdir(corpora_dir)]:\n",
    "    print('Processing {0}...'.format(fn))\n",
    "    with open(fn,'r',encoding='utf-8') as f:\n",
    "        try:\n",
    "            process_text(f.read(),model)\n",
    "        except UnicodeDecodeError:\n",
    "            print(fn)\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Generating and outputting story...')\n",
    "chapters = []\n",
    "total_length = 0\n",
    "cnum = 0\n",
    "with open('output/markov.txt','w',encoding='utf-8') as f:\n",
    "    while total_length <= 50000:\n",
    "        cnum += 1\n",
    "        print('Generating chapter ({0} words to go)...'.format(50000-total_length))\n",
    "        chapter, length = output_text(model,50000-total_length)\n",
    "        chapters.append(chapter)\n",
    "        total_length += length\n",
    "        f.write('\\n\\n   ---   Chapter {0}   ---   \\n\\n'.format(cnum))\n",
    "        f.write(chapter)\n",
    "print('Done!')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
